{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe2f443",
   "metadata": {},
   "source": [
    "# Hyperparameter search for Movielens VAE\n",
    "\n",
    "This is the ipynb I will use to actually look for the best configuration of the model and its hyperparameters\n",
    "\n",
    "\n",
    "## Input info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ad73e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leuch\\AppData\\Local\\Temp\\ipykernel_15292\\2468113692.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  raw_data = pd.read_csv(\"./data/Movielens100/u.data\", sep = None, names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [-1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "raw_data = pd.read_csv(\"./data/Movielens100/u.data\", sep = None, names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
    "raw_data = raw_data.loc[:, raw_data.columns != \"timestamp\"]\n",
    "#make indices start at 0\n",
    "raw_data[\"userId\"] -= 1\n",
    "raw_data[\"movieId\"] -= 1\n",
    "#make ratings center around 0\n",
    "raw_data[\"rating\"] -= 3\n",
    "\n",
    "# create (943, 1682) matrix of user ratings per movie\n",
    "user_ratings = pd.DataFrame(np.zeros((943,1682)))\n",
    "for i in raw_data.index:\n",
    "    user_ratings[raw_data[\"movieId\"][i]][raw_data[\"userId\"][i]] = raw_data[\"rating\"][i]\n",
    "user_ratings = user_ratings.to_numpy() \n",
    "np.random.shuffle(user_ratings)\n",
    "user_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ba4fb",
   "metadata": {},
   "source": [
    "## Begin model and tuning of Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e517bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn \n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "device = \"cuda\"\n",
    "\n",
    "class MovielensDataset(Dataset):\n",
    "    def __init__(self, data, mask_magnitude):\n",
    "        self.data = data\n",
    "        # create a mask of 0 and 1 values where half are 0 and half are 1. \n",
    "        #The ratio of masked values is something that can and should be optimized. \n",
    "        self.random_mask = np.clip((np.random.randn(1682) + mask_magnitude).round(), a_max = 1, a_min = 0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        np.random.shuffle(self.random_mask)\n",
    "        return self.data[idx] * self.random_mask, self.data[idx]\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            #Encoder\n",
    "            nn.Linear(1682, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            #Decoder\n",
    "            nn.Linear(512,1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024,1682),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # distribution parameters\n",
    "        self.fc_mu = nn.Linear(1024, 512)\n",
    "        self.fc_var = nn.Linear(1024, 512)\n",
    "        self.test_mse = 1000\n",
    "        # for the gaussian likelihood\n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "        \n",
    "        \n",
    "    def gaussian_likelihood(self, x_hat, logscale, x):\n",
    "        scale = torch.exp(logscale)\n",
    "        mean = x_hat\n",
    "        dist = torch.distributions.Normal(mean, scale)\n",
    "\n",
    "        # measure prob of seeing data under p(x|z)\n",
    "        log_pxz = dist.log_prob(x)\n",
    "        return log_pxz.sum(dim=1)\n",
    "    \n",
    "    def kl_divergence(self, z, mu, std):\n",
    "        # --------------------------\n",
    "        # Monte carlo KL divergence\n",
    "        # --------------------------\n",
    "        # 1. define the first two probabilities (in this case Normal for both)\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        # 2. get the probabilities from the equation\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        # kl\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "        return kl\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # encode x to get the mu and variance parameters\n",
    "        x_encoded = self.encoder(x)\n",
    "        mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded)\n",
    "\n",
    "        # sample z from q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        \n",
    "        #perform the kernel trick to allow for backprop through sampling\n",
    "        \n",
    "        epsilon = torch.distributions.Normal(0, 1).rsample()\n",
    "        z = mu + epsilon * std\n",
    "        # decoded\n",
    "        ratings = self.decoder(z) * 2\n",
    "        return ratings, z, mu, std\n",
    "    \n",
    "    def vae_loss(self, x_hat, x, z, mu, std):\n",
    "        # reconstruction loss\n",
    "        recon_loss = self.gaussian_likelihood(x_hat, self.log_scale, x)\n",
    "    \n",
    "        # kl\n",
    "        kl = self.kl_divergence(z, mu, std)\n",
    "\n",
    "        # elbo\n",
    "        elbo = (kl - recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "\n",
    "        return elbo\n",
    "def get_precision_and_recall(labels, predictions):\n",
    "    relevance_labels = labels > 0.5\n",
    "    \n",
    "    #To evaluate for comparison to other papers we mask by setting 20% of labels to false \n",
    "    \n",
    "    pred_relevance = predictions > 0.5\n",
    "    \n",
    "    precision_recall_fscore_support(relevance_labels.flatten().cpu(), pred_relevance.flatten().cpu(), average=\"binary\")\n",
    "\n",
    "def train(dataloader, model,  optimizer, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X, y = X.to(device).to(torch.float32), y.to(device).to(torch.float32)\n",
    "        \n",
    "        #compute prediction error\n",
    "        pred, z, mu, std = model(X)\n",
    "        loss = model.vae_loss(pred, y, z, mu, std)\n",
    "        mse_loss = mse_loss_fcn(pred, y)\n",
    "        \n",
    "        #Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 100 == 0 and batch % 64 == 0:\n",
    "            loss , current = loss.item(), (batch+1) * len(X)\n",
    "            print(\"Epoch : \" + str(epoch))\n",
    "            print(f\"loss: {loss:>7f}\")\n",
    "            print(f\"MSE loss: {mse_loss:>7f}\")\n",
    "            losses.append(loss)\n",
    "            mse_losses.append(mse_loss.item())\n",
    "\n",
    "def test(dataloader, model, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches= len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct, test_mse_loss = 0,0,0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device).to(torch.float32), y.to(device).to(torch.float32)\n",
    "            \n",
    "            \n",
    "            pred, z, mu, std = model(X)\n",
    "            test_loss += model.vae_loss(pred, y, z, mu, std).item()\n",
    "            test_mse_loss += mse_loss_fcn(pred, y)\n",
    "    test_loss /= num_batches\n",
    "    test_mse_loss /= num_batches\n",
    "    if epoch % 100 ==0:\n",
    "        test_losses.append(test_loss)\n",
    "        #Come back and actually do the accuracy where it sees if it's at least on the right side of it or compute precision/recall or something like that\n",
    "        print(f\"Test Error: \\n Avg Loss : {test_loss:>8f} \")\n",
    "        print(f\" Test MSE loss: {test_mse_loss:>7f}\")\n",
    "        test_mse_losses.append(test_mse_loss.item())\n",
    "        model.test_mse = test_mse_loss.item()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e83987b",
   "metadata": {},
   "source": [
    "## The actual search \n",
    "The following contains all of the logic you need to do to restart everything over so long as you've already loaded the ones above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61fecf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using hyperparameters : 0.0 , 0.0\n",
      "Epoch : 0\n",
      "loss: 1741.138428\n",
      "MSE loss: 0.231432\n",
      "Test Error: \n",
      " Avg Loss : 1649.350708 \n",
      " Test MSE loss: 0.125448\n",
      "New best using hyper parameters : 1.3 ,0.0 ,0.0\n",
      "Epoch : 100\n",
      "loss: 1416.155518\n",
      "MSE loss: 0.077336\n",
      "Test Error: \n",
      " Avg Loss : 1393.960083 \n",
      " Test MSE loss: 0.077230\n",
      "New best using hyper parameters : 1.3 ,0.0 ,0.0\n",
      "Epoch : 200\n",
      "loss: 1182.711792\n",
      "MSE loss: 0.063331\n",
      "Test Error: \n",
      " Avg Loss : 1152.342163 \n",
      " Test MSE loss: 0.074153\n",
      "New best using hyper parameters : 1.3 ,0.0 ,0.0\n",
      "Epoch : 300\n",
      "loss: 754.236633\n",
      "MSE loss: 0.044386\n",
      "Test Error: \n",
      " Avg Loss : 1015.983276 \n",
      " Test MSE loss: 0.070464\n",
      "New best using hyper parameters : 1.3 ,0.0 ,0.0\n",
      "Epoch : 400\n",
      "loss: 729.256592\n",
      "MSE loss: 0.038286\n",
      "Test Error: \n",
      " Avg Loss : 764.054138 \n",
      " Test MSE loss: 0.070649\n",
      "Epoch : 500\n",
      "loss: 492.864410\n",
      "MSE loss: 0.024651\n",
      "Test Error: \n",
      " Avg Loss : 616.223511 \n",
      " Test MSE loss: 0.066530\n",
      "New best using hyper parameters : 1.3 ,0.0 ,0.0\n",
      "Epoch : 600\n",
      "loss: 199.687714\n",
      "MSE loss: 0.021634\n",
      "Test Error: \n",
      " Avg Loss : 444.392426 \n",
      " Test MSE loss: 0.066100\n",
      "New best using hyper parameters : 1.3 ,0.0 ,0.0\n",
      "Epoch : 700\n",
      "loss: 33.705780\n",
      "MSE loss: 0.016907\n",
      "Test Error: \n",
      " Avg Loss : 304.267090 \n",
      " Test MSE loss: 0.066423\n",
      "Epoch : 800\n",
      "loss: -262.607849\n",
      "MSE loss: 0.016502\n",
      "Test Error: \n",
      " Avg Loss : 187.688507 \n",
      " Test MSE loss: 0.070613\n",
      "Epoch : 900\n",
      "loss: -419.900909\n",
      "MSE loss: 0.009200\n",
      "Test Error: \n",
      " Avg Loss : 125.610542 \n",
      " Test MSE loss: 0.067116\n",
      "Epoch : 1000\n",
      "loss: -614.631714\n",
      "MSE loss: 0.007509\n",
      "Test Error: \n",
      " Avg Loss : 155.374268 \n",
      " Test MSE loss: 0.066783\n",
      "Epoch : 1100\n",
      "loss: -818.769043\n",
      "MSE loss: 0.007520\n",
      "Test Error: \n",
      " Avg Loss : 253.764389 \n",
      " Test MSE loss: 0.070931\n",
      "Epoch : 1200\n",
      "loss: -1043.941406\n",
      "MSE loss: 0.006338\n",
      "Test Error: \n",
      " Avg Loss : 358.083221 \n",
      " Test MSE loss: 0.067795\n",
      "Epoch : 1300\n",
      "loss: -1356.157227\n",
      "MSE loss: 0.005943\n",
      "Test Error: \n",
      " Avg Loss : 662.510864 \n",
      " Test MSE loss: 0.070479\n",
      "Epoch : 1400\n",
      "loss: -1481.929443\n",
      "MSE loss: 0.003688\n",
      "Test Error: \n",
      " Avg Loss : 1022.346680 \n",
      " Test MSE loss: 0.069041\n",
      "Epoch : 1500\n",
      "loss: -1664.098389\n",
      "MSE loss: 0.002984\n",
      "Test Error: \n",
      " Avg Loss : 1584.604614 \n",
      " Test MSE loss: 0.067974\n",
      "Epoch : 1600\n",
      "loss: -1465.998901\n",
      "MSE loss: 0.008703\n",
      "Test Error: \n",
      " Avg Loss : 5262.364258 \n",
      " Test MSE loss: 0.115708\n",
      "Epoch : 1700\n",
      "loss: -2101.115723\n",
      "MSE loss: 0.002776\n",
      "Test Error: \n",
      " Avg Loss : 3282.052002 \n",
      " Test MSE loss: 0.068839\n",
      "Epoch : 1800\n",
      "loss: -2142.428223\n",
      "MSE loss: 0.002156\n",
      "Test Error: \n",
      " Avg Loss : 4418.505371 \n",
      " Test MSE loss: 0.068490\n",
      "Epoch : 1900\n",
      "loss: -2173.008789\n",
      "MSE loss: 0.002037\n",
      "Test Error: \n",
      " Avg Loss : 5127.518555 \n",
      " Test MSE loss: 0.069067\n",
      "using hyperparameters : 0.0 , 0.1\n",
      "Epoch : 0\n",
      "loss: 1843.107544\n",
      "MSE loss: 0.353244\n",
      "Test Error: \n",
      " Avg Loss : 1666.070923 \n",
      " Test MSE loss: 0.134291\n",
      "Epoch : 100\n",
      "loss: 1369.278442\n",
      "MSE loss: 0.068785\n",
      "Test Error: \n",
      " Avg Loss : 1374.152100 \n",
      " Test MSE loss: 0.076146\n",
      "Epoch : 200\n",
      "loss: 1158.710815\n",
      "MSE loss: 0.055070\n",
      "Test Error: \n",
      " Avg Loss : 1253.491577 \n",
      " Test MSE loss: 0.119757\n",
      "Epoch : 300\n",
      "loss: 949.882996\n",
      "MSE loss: 0.052947\n",
      "Test Error: \n",
      " Avg Loss : 972.273132 \n",
      " Test MSE loss: 0.071359\n",
      "Epoch : 400\n",
      "loss: 721.722778\n",
      "MSE loss: 0.039954\n",
      "Test Error: \n",
      " Avg Loss : 793.424744 \n",
      " Test MSE loss: 0.070664\n",
      "Epoch : 500\n",
      "loss: 466.182068\n",
      "MSE loss: 0.033138\n",
      "Test Error: \n",
      " Avg Loss : 575.473206 \n",
      " Test MSE loss: 0.068843\n",
      "Epoch : 600\n",
      "loss: 276.942017\n",
      "MSE loss: 0.029571\n",
      "Test Error: \n",
      " Avg Loss : 502.780060 \n",
      " Test MSE loss: 0.071147\n",
      "Epoch : 700\n",
      "loss: 69.012154\n",
      "MSE loss: 0.023267\n",
      "Test Error: \n",
      " Avg Loss : 324.127563 \n",
      " Test MSE loss: 0.070215\n",
      "Epoch : 800\n",
      "loss: -193.452713\n",
      "MSE loss: 0.022183\n",
      "Test Error: \n",
      " Avg Loss : 160.923141 \n",
      " Test MSE loss: 0.070377\n",
      "Epoch : 900\n",
      "loss: -336.958252\n",
      "MSE loss: 0.017867\n",
      "Test Error: \n",
      " Avg Loss : 176.421249 \n",
      " Test MSE loss: 0.070452\n",
      "Epoch : 1000\n",
      "loss: -504.354553\n",
      "MSE loss: 0.019472\n",
      "Test Error: \n",
      " Avg Loss : 155.941360 \n",
      " Test MSE loss: 0.072044\n",
      "Epoch : 1100\n",
      "loss: -745.371582\n",
      "MSE loss: 0.015934\n",
      "Test Error: \n",
      " Avg Loss : 241.989227 \n",
      " Test MSE loss: 0.071452\n",
      "Epoch : 1200\n",
      "loss: -832.004028\n",
      "MSE loss: 0.014920\n",
      "Test Error: \n",
      " Avg Loss : 413.904053 \n",
      " Test MSE loss: 0.072680\n",
      "Epoch : 1300\n",
      "loss: -854.321655\n",
      "MSE loss: 0.017736\n",
      "Test Error: \n",
      " Avg Loss : 709.235779 \n",
      " Test MSE loss: 0.073153\n",
      "Epoch : 1400\n",
      "loss: -989.030945\n",
      "MSE loss: 0.014797\n",
      "Test Error: \n",
      " Avg Loss : 956.052734 \n",
      " Test MSE loss: 0.070842\n",
      "Epoch : 1500\n",
      "loss: -1147.533447\n",
      "MSE loss: 0.013797\n",
      "Test Error: \n",
      " Avg Loss : 1364.494873 \n",
      " Test MSE loss: 0.070616\n",
      "Epoch : 1600\n",
      "loss: -1163.110107\n",
      "MSE loss: 0.013597\n",
      "Test Error: \n",
      " Avg Loss : 1811.822754 \n",
      " Test MSE loss: 0.070668\n",
      "Epoch : 1700\n",
      "loss: -676.133911\n",
      "MSE loss: 0.018401\n",
      "Test Error: \n",
      " Avg Loss : 2496.290039 \n",
      " Test MSE loss: 0.074177\n",
      "Epoch : 1800\n",
      "loss: -1454.742676\n",
      "MSE loss: 0.012441\n",
      "Test Error: \n",
      " Avg Loss : 2289.922363 \n",
      " Test MSE loss: 0.072664\n",
      "Epoch : 1900\n",
      "loss: -1304.199341\n",
      "MSE loss: 0.011263\n",
      "Test Error: \n",
      " Avg Loss : 2394.116211 \n",
      " Test MSE loss: 0.069742\n",
      "using hyperparameters : 0.0 , 0.2\n",
      "Epoch : 0\n",
      "loss: 1696.530273\n",
      "MSE loss: 0.179118\n",
      "Test Error: \n",
      " Avg Loss : 1631.035278 \n",
      " Test MSE loss: 0.104672\n",
      "Epoch : 100\n",
      "loss: 1408.675171\n",
      "MSE loss: 0.094606\n",
      "Test Error: \n",
      " Avg Loss : 1454.186646 \n",
      " Test MSE loss: 0.082668\n",
      "Epoch : 200\n",
      "loss: 1182.993164\n",
      "MSE loss: 0.065627\n",
      "Test Error: \n",
      " Avg Loss : 1174.688843 \n",
      " Test MSE loss: 0.074633\n",
      "Epoch : 300\n",
      "loss: 953.678345\n",
      "MSE loss: 0.051754\n",
      "Test Error: \n",
      " Avg Loss : 964.960815 \n",
      " Test MSE loss: 0.072748\n",
      "Epoch : 400\n",
      "loss: 733.116821\n",
      "MSE loss: 0.051602\n",
      "Test Error: \n",
      " Avg Loss : 820.072021 \n",
      " Test MSE loss: 0.079934\n",
      "Epoch : 500\n",
      "loss: 535.652588\n",
      "MSE loss: 0.040748\n",
      "Test Error: \n",
      " Avg Loss : 620.029602 \n",
      " Test MSE loss: 0.071895\n",
      "Epoch : 600\n",
      "loss: 345.154297\n",
      "MSE loss: 0.037939\n",
      "Test Error: \n",
      " Avg Loss : 465.505371 \n",
      " Test MSE loss: 0.072586\n",
      "Epoch : 700\n",
      "loss: 127.173782\n",
      "MSE loss: 0.032548\n",
      "Test Error: \n",
      " Avg Loss : 334.763519 \n",
      " Test MSE loss: 0.070805\n",
      "Epoch : 800\n",
      "loss: -61.461021\n",
      "MSE loss: 0.029100\n",
      "Test Error: \n",
      " Avg Loss : 252.958237 \n",
      " Test MSE loss: 0.072651\n",
      "Epoch : 900\n",
      "loss: -236.091919\n",
      "MSE loss: 0.025308\n",
      "Test Error: \n",
      " Avg Loss : 177.547363 \n",
      " Test MSE loss: 0.071853\n",
      "Epoch : 1000\n",
      "loss: -417.909119\n",
      "MSE loss: 0.022452\n",
      "Test Error: \n",
      " Avg Loss : 160.379471 \n",
      " Test MSE loss: 0.071586\n",
      "Epoch : 1100\n",
      "loss: -575.725220\n",
      "MSE loss: 0.023193\n",
      "Test Error: \n",
      " Avg Loss : 270.618561 \n",
      " Test MSE loss: 0.073117\n",
      "Epoch : 1200\n",
      "loss: -682.867615\n",
      "MSE loss: 0.020366\n",
      "Test Error: \n",
      " Avg Loss : 416.576172 \n",
      " Test MSE loss: 0.073126\n",
      "Epoch : 1300\n",
      "loss: -780.315063\n",
      "MSE loss: 0.019341\n",
      "Test Error: \n",
      " Avg Loss : 607.453552 \n",
      " Test MSE loss: 0.072326\n",
      "Epoch : 1400\n",
      "loss: -896.087463\n",
      "MSE loss: 0.018306\n",
      "Test Error: \n",
      " Avg Loss : 840.522034 \n",
      " Test MSE loss: 0.071458\n",
      "Epoch : 1500\n",
      "loss: -863.655273\n",
      "MSE loss: 0.018674\n",
      "Test Error: \n",
      " Avg Loss : 991.002625 \n",
      " Test MSE loss: 0.070194\n",
      "Epoch : 1600\n",
      "loss: -937.626099\n",
      "MSE loss: 0.018394\n",
      "Test Error: \n",
      " Avg Loss : 1318.450073 \n",
      " Test MSE loss: 0.071681\n",
      "Epoch : 1700\n",
      "loss: -1096.319702\n",
      "MSE loss: 0.017438\n",
      "Test Error: \n",
      " Avg Loss : 1404.330444 \n",
      " Test MSE loss: 0.070220\n",
      "Epoch : 1800\n",
      "loss: -981.089355\n",
      "MSE loss: 0.017134\n",
      "Test Error: \n",
      " Avg Loss : 1453.961182 \n",
      " Test MSE loss: 0.070283\n",
      "Epoch : 1900\n",
      "loss: -988.153381\n",
      "MSE loss: 0.016481\n",
      "Test Error: \n",
      " Avg Loss : 1449.161255 \n",
      " Test MSE loss: 0.069472\n",
      "using hyperparameters : 0.0 , 0.3\n",
      "Epoch : 0\n",
      "loss: 2180.244141\n",
      "MSE loss: 0.754233\n",
      "Test Error: \n",
      " Avg Loss : 1637.990601 \n",
      " Test MSE loss: 0.114792\n",
      "Epoch : 100\n",
      "loss: 1384.831909\n",
      "MSE loss: 0.067693\n",
      "Test Error: \n",
      " Avg Loss : 1393.441406 \n",
      " Test MSE loss: 0.080263\n",
      "Epoch : 200\n",
      "loss: 1177.484253\n",
      "MSE loss: 0.068546\n",
      "Test Error: \n",
      " Avg Loss : 1189.228760 \n",
      " Test MSE loss: 0.075530\n",
      "Epoch : 300\n",
      "loss: 984.267151\n",
      "MSE loss: 0.065013\n",
      "Test Error: \n",
      " Avg Loss : 999.684448 \n",
      " Test MSE loss: 0.074378\n",
      "Epoch : 400\n",
      "loss: 789.979553\n",
      "MSE loss: 0.067764\n",
      "Test Error: \n",
      " Avg Loss : 800.239502 \n",
      " Test MSE loss: 0.074705\n",
      "Epoch : 500\n",
      "loss: 567.541016\n",
      "MSE loss: 0.053291\n",
      "Test Error: \n",
      " Avg Loss : 680.622803 \n",
      " Test MSE loss: 0.088121\n",
      "Epoch : 600\n",
      "loss: 326.026306\n",
      "MSE loss: 0.042211\n",
      "Test Error: \n",
      " Avg Loss : 510.197144 \n",
      " Test MSE loss: 0.077182\n",
      "Epoch : 700\n",
      "loss: 147.411926\n",
      "MSE loss: 0.037232\n",
      "Test Error: \n",
      " Avg Loss : 345.689392 \n",
      " Test MSE loss: 0.073348\n",
      "Epoch : 800\n",
      "loss: -32.041561\n",
      "MSE loss: 0.034350\n",
      "Test Error: \n",
      " Avg Loss : 243.995972 \n",
      " Test MSE loss: 0.072395\n",
      "Epoch : 900\n",
      "loss: -227.553131\n",
      "MSE loss: 0.030520\n",
      "Test Error: \n",
      " Avg Loss : 219.758301 \n",
      " Test MSE loss: 0.074959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1000\n",
      "loss: -308.754639\n",
      "MSE loss: 0.030018\n",
      "Test Error: \n",
      " Avg Loss : 219.580978 \n",
      " Test MSE loss: 0.073759\n",
      "Epoch : 1100\n",
      "loss: -480.925751\n",
      "MSE loss: 0.027382\n",
      "Test Error: \n",
      " Avg Loss : 277.764771 \n",
      " Test MSE loss: 0.073836\n",
      "Epoch : 1200\n",
      "loss: -581.954346\n",
      "MSE loss: 0.024163\n",
      "Test Error: \n",
      " Avg Loss : 397.335999 \n",
      " Test MSE loss: 0.073827\n",
      "Epoch : 1300\n",
      "loss: -674.286621\n",
      "MSE loss: 0.027515\n",
      "Test Error: \n",
      " Avg Loss : 495.056000 \n",
      " Test MSE loss: 0.072181\n",
      "Epoch : 1400\n",
      "loss: -705.662964\n",
      "MSE loss: 0.023183\n",
      "Test Error: \n",
      " Avg Loss : 728.328796 \n",
      " Test MSE loss: 0.074381\n",
      "Epoch : 1500\n",
      "loss: -745.179443\n",
      "MSE loss: 0.022787\n",
      "Test Error: \n",
      " Avg Loss : 786.323242 \n",
      " Test MSE loss: 0.071307\n",
      "Epoch : 1600\n",
      "loss: -645.223755\n",
      "MSE loss: 0.024999\n",
      "Test Error: \n",
      " Avg Loss : 806.147888 \n",
      " Test MSE loss: 0.070645\n",
      "Epoch : 1700\n",
      "loss: -829.159180\n",
      "MSE loss: 0.020301\n",
      "Test Error: \n",
      " Avg Loss : 896.076050 \n",
      " Test MSE loss: 0.070334\n",
      "Epoch : 1800\n",
      "loss: -819.884705\n",
      "MSE loss: 0.022095\n",
      "Test Error: \n",
      " Avg Loss : 943.168884 \n",
      " Test MSE loss: 0.070273\n",
      "Epoch : 1900\n",
      "loss: -819.098267\n",
      "MSE loss: 0.020487\n",
      "Test Error: \n",
      " Avg Loss : 1003.237610 \n",
      " Test MSE loss: 0.071243\n",
      "using hyperparameters : 0.1 , 0.0\n",
      "Epoch : 0\n",
      "loss: 1714.940674\n",
      "MSE loss: 0.200350\n",
      "Test Error: \n",
      " Avg Loss : 1626.611084 \n",
      " Test MSE loss: 0.095545\n",
      "Epoch : 100\n",
      "loss: 1403.234375\n",
      "MSE loss: 0.081950\n",
      "Test Error: \n",
      " Avg Loss : 1391.399292 \n",
      " Test MSE loss: 0.078817\n",
      "Epoch : 200\n",
      "loss: 1130.180908\n",
      "MSE loss: 0.079976\n",
      "Test Error: \n",
      " Avg Loss : 1199.151733 \n",
      " Test MSE loss: 0.075581\n",
      "Epoch : 300\n",
      "loss: 990.313354\n",
      "MSE loss: 0.068153\n",
      "Test Error: \n",
      " Avg Loss : 971.352661 \n",
      " Test MSE loss: 0.076555\n",
      "Epoch : 400\n",
      "loss: 807.655396\n",
      "MSE loss: 0.071354\n",
      "Test Error: \n",
      " Avg Loss : 806.204651 \n",
      " Test MSE loss: 0.073579\n",
      "Epoch : 500\n",
      "loss: 617.260864\n",
      "MSE loss: 0.065074\n",
      "Test Error: \n",
      " Avg Loss : 677.666016 \n",
      " Test MSE loss: 0.084298\n",
      "Epoch : 600\n",
      "loss: 398.316772\n",
      "MSE loss: 0.053176\n",
      "Test Error: \n",
      " Avg Loss : 483.769409 \n",
      " Test MSE loss: 0.073382\n",
      "Epoch : 700\n",
      "loss: 242.414383\n",
      "MSE loss: 0.051493\n",
      "Test Error: \n",
      " Avg Loss : 364.500214 \n",
      " Test MSE loss: 0.073302\n",
      "Epoch : 800\n",
      "loss: 48.848320\n",
      "MSE loss: 0.040150\n",
      "Test Error: \n",
      " Avg Loss : 247.175232 \n",
      " Test MSE loss: 0.074191\n",
      "Epoch : 900\n",
      "loss: -197.931274\n",
      "MSE loss: 0.029952\n",
      "Test Error: \n",
      " Avg Loss : 233.514297 \n",
      " Test MSE loss: 0.074123\n",
      "Epoch : 1000\n",
      "loss: -394.397156\n",
      "MSE loss: 0.026181\n",
      "Test Error: \n",
      " Avg Loss : 190.969177 \n",
      " Test MSE loss: 0.075048\n",
      "Epoch : 1100\n",
      "loss: -260.135803\n",
      "MSE loss: 0.034842\n",
      "Test Error: \n",
      " Avg Loss : 333.467102 \n",
      " Test MSE loss: 0.079794\n",
      "Epoch : 1200\n",
      "loss: -683.750671\n",
      "MSE loss: 0.020786\n",
      "Test Error: \n",
      " Avg Loss : 552.864380 \n",
      " Test MSE loss: 0.078444\n",
      "Epoch : 1300\n",
      "loss: -886.960571\n",
      "MSE loss: 0.016177\n",
      "Test Error: \n",
      " Avg Loss : 800.227478 \n",
      " Test MSE loss: 0.077011\n",
      "Epoch : 1400\n",
      "loss: -1085.520020\n",
      "MSE loss: 0.012987\n",
      "Test Error: \n",
      " Avg Loss : 1224.786865 \n",
      " Test MSE loss: 0.077368\n",
      "Epoch : 1500\n",
      "loss: -1168.009033\n",
      "MSE loss: 0.011793\n",
      "Test Error: \n",
      " Avg Loss : 1609.604980 \n",
      " Test MSE loss: 0.078267\n",
      "Epoch : 1600\n",
      "loss: -1204.648071\n",
      "MSE loss: 0.012163\n",
      "Test Error: \n",
      " Avg Loss : 2203.442871 \n",
      " Test MSE loss: 0.077921\n",
      "Epoch : 1700\n",
      "loss: -1369.010986\n",
      "MSE loss: 0.010271\n",
      "Test Error: \n",
      " Avg Loss : 3020.929199 \n",
      " Test MSE loss: 0.078785\n",
      "Epoch : 1800\n",
      "loss: -1450.517822\n",
      "MSE loss: 0.008850\n",
      "Test Error: \n",
      " Avg Loss : 3552.588135 \n",
      " Test MSE loss: 0.078381\n",
      "Epoch : 1900\n",
      "loss: -1639.776611\n",
      "MSE loss: 0.006831\n",
      "Test Error: \n",
      " Avg Loss : 4568.726562 \n",
      " Test MSE loss: 0.079258\n",
      "using hyperparameters : 0.1 , 0.1\n",
      "Epoch : 0\n",
      "loss: 1668.443359\n",
      "MSE loss: 0.145355\n",
      "Test Error: \n",
      " Avg Loss : 1628.210449 \n",
      " Test MSE loss: 0.097552\n",
      "Epoch : 100\n",
      "loss: 1414.212402\n",
      "MSE loss: 0.079320\n",
      "Test Error: \n",
      " Avg Loss : 1393.225342 \n",
      " Test MSE loss: 0.078847\n",
      "Epoch : 200\n",
      "loss: 1178.485596\n",
      "MSE loss: 0.075186\n",
      "Test Error: \n",
      " Avg Loss : 1176.564331 \n",
      " Test MSE loss: 0.075896\n",
      "Epoch : 300\n",
      "loss: 986.970093\n",
      "MSE loss: 0.074863\n",
      "Test Error: \n",
      " Avg Loss : 976.501099 \n",
      " Test MSE loss: 0.074783\n",
      "Epoch : 400\n",
      "loss: 795.322327\n",
      "MSE loss: 0.067534\n",
      "Test Error: \n",
      " Avg Loss : 807.206177 \n",
      " Test MSE loss: 0.074286\n",
      "Epoch : 500\n",
      "loss: 602.786499\n",
      "MSE loss: 0.061358\n",
      "Test Error: \n",
      " Avg Loss : 634.154358 \n",
      " Test MSE loss: 0.073323\n",
      "Epoch : 600\n",
      "loss: 438.733978\n",
      "MSE loss: 0.059142\n",
      "Test Error: \n",
      " Avg Loss : 464.545898 \n",
      " Test MSE loss: 0.084434\n",
      "Epoch : 700\n",
      "loss: 304.517456\n",
      "MSE loss: 0.059122\n",
      "Test Error: \n",
      " Avg Loss : 366.774353 \n",
      " Test MSE loss: 0.073195\n",
      "Epoch : 800\n",
      "loss: 87.573517\n",
      "MSE loss: 0.045687\n",
      "Test Error: \n",
      " Avg Loss : 252.236862 \n",
      " Test MSE loss: 0.072913\n",
      "Epoch : 900\n",
      "loss: -127.775230\n",
      "MSE loss: 0.041244\n",
      "Test Error: \n",
      " Avg Loss : 213.692795 \n",
      " Test MSE loss: 0.072468\n",
      "Epoch : 1000\n",
      "loss: -234.617188\n",
      "MSE loss: 0.038103\n",
      "Test Error: \n",
      " Avg Loss : 195.891144 \n",
      " Test MSE loss: 0.072551\n",
      "Epoch : 1100\n",
      "loss: -377.671326\n",
      "MSE loss: 0.031686\n",
      "Test Error: \n",
      " Avg Loss : 248.539383 \n",
      " Test MSE loss: 0.073086\n",
      "Epoch : 1200\n",
      "loss: -403.601074\n",
      "MSE loss: 0.033141\n",
      "Test Error: \n",
      " Avg Loss : 385.890686 \n",
      " Test MSE loss: 0.072862\n",
      "Epoch : 1300\n",
      "loss: -588.755005\n",
      "MSE loss: 0.026520\n",
      "Test Error: \n",
      " Avg Loss : 492.091278 \n",
      " Test MSE loss: 0.075062\n",
      "Epoch : 1400\n",
      "loss: -711.599487\n",
      "MSE loss: 0.022986\n",
      "Test Error: \n",
      " Avg Loss : 807.087219 \n",
      " Test MSE loss: 0.073696\n",
      "Epoch : 1500\n",
      "loss: -809.499146\n",
      "MSE loss: 0.020708\n",
      "Test Error: \n",
      " Avg Loss : 1045.411499 \n",
      " Test MSE loss: 0.073813\n",
      "Epoch : 1600\n",
      "loss: -892.601196\n",
      "MSE loss: 0.019470\n",
      "Test Error: \n",
      " Avg Loss : 1194.809204 \n",
      " Test MSE loss: 0.074911\n",
      "Epoch : 1700\n",
      "loss: -885.652832\n",
      "MSE loss: 0.019208\n",
      "Test Error: \n",
      " Avg Loss : 1481.246826 \n",
      " Test MSE loss: 0.075724\n",
      "Epoch : 1800\n",
      "loss: -967.498779\n",
      "MSE loss: 0.017226\n",
      "Test Error: \n",
      " Avg Loss : 1619.960571 \n",
      " Test MSE loss: 0.074218\n",
      "Epoch : 1900\n",
      "loss: -999.284668\n",
      "MSE loss: 0.016244\n",
      "Test Error: \n",
      " Avg Loss : 1699.766479 \n",
      " Test MSE loss: 0.075440\n",
      "using hyperparameters : 0.1 , 0.2\n",
      "Epoch : 0\n",
      "loss: 1684.832642\n",
      "MSE loss: 0.164967\n",
      "Test Error: \n",
      " Avg Loss : 1764.245728 \n",
      " Test MSE loss: 0.235529\n",
      "Epoch : 100\n",
      "loss: 1392.814575\n",
      "MSE loss: 0.073396\n",
      "Test Error: \n",
      " Avg Loss : 1399.045898 \n",
      " Test MSE loss: 0.080342\n",
      "Epoch : 200\n",
      "loss: 1187.221191\n",
      "MSE loss: 0.069789\n",
      "Test Error: \n",
      " Avg Loss : 1183.223389 \n",
      " Test MSE loss: 0.077912\n",
      "Epoch : 300\n",
      "loss: 979.747559\n",
      "MSE loss: 0.067830\n",
      "Test Error: \n",
      " Avg Loss : 986.659668 \n",
      " Test MSE loss: 0.075126\n",
      "Epoch : 400\n",
      "loss: 766.890625\n",
      "MSE loss: 0.060432\n",
      "Test Error: \n",
      " Avg Loss : 819.347534 \n",
      " Test MSE loss: 0.076753\n",
      "Epoch : 500\n",
      "loss: 995.488708\n",
      "MSE loss: 0.173447\n",
      "Test Error: \n",
      " Avg Loss : 733.691406 \n",
      " Test MSE loss: 0.100810\n",
      "Epoch : 600\n",
      "loss: 468.228119\n",
      "MSE loss: 0.066845\n",
      "Test Error: \n",
      " Avg Loss : 495.638733 \n",
      " Test MSE loss: 0.073150\n",
      "Epoch : 700\n",
      "loss: 321.415344\n",
      "MSE loss: 0.056562\n",
      "Test Error: \n",
      " Avg Loss : 352.678497 \n",
      " Test MSE loss: 0.073010\n",
      "Epoch : 800\n",
      "loss: 125.520042\n",
      "MSE loss: 0.053199\n",
      "Test Error: \n",
      " Avg Loss : 259.404297 \n",
      " Test MSE loss: 0.072587\n",
      "Epoch : 900\n",
      "loss: 24.949448\n",
      "MSE loss: 0.051665\n",
      "Test Error: \n",
      " Avg Loss : 207.756409 \n",
      " Test MSE loss: 0.072481\n",
      "Epoch : 1000\n",
      "loss: -135.571289\n",
      "MSE loss: 0.042677\n",
      "Test Error: \n",
      " Avg Loss : 203.613907 \n",
      " Test MSE loss: 0.072701\n",
      "Epoch : 1100\n",
      "loss: -242.735626\n",
      "MSE loss: 0.040503\n",
      "Test Error: \n",
      " Avg Loss : 235.168472 \n",
      " Test MSE loss: 0.072908\n",
      "Epoch : 1200\n",
      "loss: -432.471710\n",
      "MSE loss: 0.031813\n",
      "Test Error: \n",
      " Avg Loss : 326.449707 \n",
      " Test MSE loss: 0.073419\n",
      "Epoch : 1300\n",
      "loss: -481.106201\n",
      "MSE loss: 0.031447\n",
      "Test Error: \n",
      " Avg Loss : 468.078979 \n",
      " Test MSE loss: 0.073230\n",
      "Epoch : 1400\n",
      "loss: -558.439209\n",
      "MSE loss: 0.029108\n",
      "Test Error: \n",
      " Avg Loss : 616.404724 \n",
      " Test MSE loss: 0.073766\n",
      "Epoch : 1500\n",
      "loss: -617.688110\n",
      "MSE loss: 0.026154\n",
      "Test Error: \n",
      " Avg Loss : 734.526367 \n",
      " Test MSE loss: 0.073662\n",
      "Epoch : 1600\n",
      "loss: -668.688599\n",
      "MSE loss: 0.024228\n",
      "Test Error: \n",
      " Avg Loss : 851.649841 \n",
      " Test MSE loss: 0.075744\n",
      "Epoch : 1700\n",
      "loss: -629.078613\n",
      "MSE loss: 0.025670\n",
      "Test Error: \n",
      " Avg Loss : 928.566284 \n",
      " Test MSE loss: 0.073827\n",
      "Epoch : 1800\n",
      "loss: -692.936768\n",
      "MSE loss: 0.024917\n",
      "Test Error: \n",
      " Avg Loss : 996.988708 \n",
      " Test MSE loss: 0.073467\n",
      "Epoch : 1900\n",
      "loss: -674.096191\n",
      "MSE loss: 0.028229\n",
      "Test Error: \n",
      " Avg Loss : 947.524719 \n",
      " Test MSE loss: 0.074583\n",
      "using hyperparameters : 0.1 , 0.3\n",
      "Epoch : 0\n",
      "loss: 1744.789429\n",
      "MSE loss: 0.236115\n",
      "Test Error: \n",
      " Avg Loss : 1630.678101 \n",
      " Test MSE loss: 0.100317\n",
      "Epoch : 100\n",
      "loss: 1384.158203\n",
      "MSE loss: 0.067055\n",
      "Test Error: \n",
      " Avg Loss : 1397.677734 \n",
      " Test MSE loss: 0.081659\n",
      "Epoch : 200\n",
      "loss: 1181.613037\n",
      "MSE loss: 0.065129\n",
      "Test Error: \n",
      " Avg Loss : 1191.295288 \n",
      " Test MSE loss: 0.078942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 300\n",
      "loss: 988.649536\n",
      "MSE loss: 0.078411\n",
      "Test Error: \n",
      " Avg Loss : 992.081726 \n",
      " Test MSE loss: 0.077245\n",
      "Epoch : 400\n",
      "loss: 795.580444\n",
      "MSE loss: 0.070373\n",
      "Test Error: \n",
      " Avg Loss : 793.786072 \n",
      " Test MSE loss: 0.075460\n",
      "Epoch : 500\n",
      "loss: 628.964905\n",
      "MSE loss: 0.071212\n",
      "Test Error: \n",
      " Avg Loss : 644.152771 \n",
      " Test MSE loss: 0.074968\n",
      "Epoch : 600\n",
      "loss: 462.297974\n",
      "MSE loss: 0.061746\n",
      "Test Error: \n",
      " Avg Loss : 492.159058 \n",
      " Test MSE loss: 0.073587\n",
      "Epoch : 700\n",
      "loss: 280.462799\n",
      "MSE loss: 0.057032\n",
      "Test Error: \n",
      " Avg Loss : 352.860382 \n",
      " Test MSE loss: 0.073295\n",
      "Epoch : 800\n",
      "loss: 155.525787\n",
      "MSE loss: 0.055718\n",
      "Test Error: \n",
      " Avg Loss : 244.576355 \n",
      " Test MSE loss: 0.072940\n",
      "Epoch : 900\n",
      "loss: 28.697838\n",
      "MSE loss: 0.052172\n",
      "Test Error: \n",
      " Avg Loss : 195.383881 \n",
      " Test MSE loss: 0.072596\n",
      "Epoch : 1000\n",
      "loss: -74.247124\n",
      "MSE loss: 0.047164\n",
      "Test Error: \n",
      " Avg Loss : 220.156631 \n",
      " Test MSE loss: 0.072499\n",
      "Epoch : 1100\n",
      "loss: -125.392502\n",
      "MSE loss: 0.046250\n",
      "Test Error: \n",
      " Avg Loss : 231.176239 \n",
      " Test MSE loss: 0.072423\n",
      "Epoch : 1200\n",
      "loss: -258.564941\n",
      "MSE loss: 0.040733\n",
      "Test Error: \n",
      " Avg Loss : 275.214417 \n",
      " Test MSE loss: 0.072782\n",
      "Epoch : 1300\n",
      "loss: -308.319763\n",
      "MSE loss: 0.038569\n",
      "Test Error: \n",
      " Avg Loss : 381.854614 \n",
      " Test MSE loss: 0.072641\n",
      "Epoch : 1400\n",
      "loss: -430.439453\n",
      "MSE loss: 0.032435\n",
      "Test Error: \n",
      " Avg Loss : 462.339783 \n",
      " Test MSE loss: 0.073449\n",
      "Epoch : 1500\n",
      "loss: -492.695190\n",
      "MSE loss: 0.029883\n",
      "Test Error: \n",
      " Avg Loss : 590.529175 \n",
      " Test MSE loss: 0.075170\n",
      "Epoch : 1600\n",
      "loss: -485.655273\n",
      "MSE loss: 0.031783\n",
      "Test Error: \n",
      " Avg Loss : 582.451599 \n",
      " Test MSE loss: 0.072992\n",
      "Epoch : 1700\n",
      "loss: -554.057495\n",
      "MSE loss: 0.029039\n",
      "Test Error: \n",
      " Avg Loss : 682.424683 \n",
      " Test MSE loss: 0.074110\n",
      "Epoch : 1800\n",
      "loss: -610.657227\n",
      "MSE loss: 0.026409\n",
      "Test Error: \n",
      " Avg Loss : 703.893433 \n",
      " Test MSE loss: 0.073419\n",
      "Epoch : 1900\n",
      "loss: -501.277893\n",
      "MSE loss: 0.029333\n",
      "Test Error: \n",
      " Avg Loss : 753.076843 \n",
      " Test MSE loss: 0.073447\n",
      "using hyperparameters : 0.2 , 0.0\n",
      "Epoch : 0\n",
      "loss: 1702.249756\n",
      "MSE loss: 0.185603\n",
      "Test Error: \n",
      " Avg Loss : 1619.479370 \n",
      " Test MSE loss: 0.095306\n",
      "Epoch : 100\n",
      "loss: 1416.963989\n",
      "MSE loss: 0.101546\n",
      "Test Error: \n",
      " Avg Loss : 1401.521484 \n",
      " Test MSE loss: 0.078839\n",
      "Epoch : 200\n",
      "loss: 1191.333252\n",
      "MSE loss: 0.074182\n",
      "Test Error: \n",
      " Avg Loss : 1183.794922 \n",
      " Test MSE loss: 0.077956\n",
      "Epoch : 300\n",
      "loss: 997.535645\n",
      "MSE loss: 0.072419\n",
      "Test Error: \n",
      " Avg Loss : 998.673157 \n",
      " Test MSE loss: 0.077054\n",
      "Epoch : 400\n",
      "loss: 812.126099\n",
      "MSE loss: 0.068450\n",
      "Test Error: \n",
      " Avg Loss : 808.090759 \n",
      " Test MSE loss: 0.076748\n",
      "Epoch : 500\n",
      "loss: 639.742310\n",
      "MSE loss: 0.074401\n",
      "Test Error: \n",
      " Avg Loss : 649.271851 \n",
      " Test MSE loss: 0.076315\n",
      "Epoch : 600\n",
      "loss: 522.304932\n",
      "MSE loss: 0.076971\n",
      "Test Error: \n",
      " Avg Loss : 509.704376 \n",
      " Test MSE loss: 0.075070\n",
      "Epoch : 700\n",
      "loss: 337.698669\n",
      "MSE loss: 0.066576\n",
      "Test Error: \n",
      " Avg Loss : 337.259888 \n",
      " Test MSE loss: 0.074242\n",
      "Epoch : 800\n",
      "loss: 167.083572\n",
      "MSE loss: 0.054974\n",
      "Test Error: \n",
      " Avg Loss : 296.015656 \n",
      " Test MSE loss: 0.074033\n",
      "Epoch : 900\n",
      "loss: 123.385742\n",
      "MSE loss: 0.057144\n",
      "Test Error: \n",
      " Avg Loss : 254.991486 \n",
      " Test MSE loss: 0.074394\n",
      "Epoch : 1000\n",
      "loss: -87.094360\n",
      "MSE loss: 0.047350\n",
      "Test Error: \n",
      " Avg Loss : 228.102188 \n",
      " Test MSE loss: 0.076144\n",
      "Epoch : 1100\n",
      "loss: -190.051208\n",
      "MSE loss: 0.043816\n",
      "Test Error: \n",
      " Avg Loss : 308.493774 \n",
      " Test MSE loss: 0.077849\n",
      "Epoch : 1200\n",
      "loss: -368.637451\n",
      "MSE loss: 0.035179\n",
      "Test Error: \n",
      " Avg Loss : 412.299683 \n",
      " Test MSE loss: 0.078621\n",
      "Epoch : 1300\n",
      "loss: -523.630737\n",
      "MSE loss: 0.029032\n",
      "Test Error: \n",
      " Avg Loss : 620.071960 \n",
      " Test MSE loss: 0.080314\n",
      "Epoch : 1400\n",
      "loss: -607.186096\n",
      "MSE loss: 0.027518\n",
      "Test Error: \n",
      " Avg Loss : 863.457581 \n",
      " Test MSE loss: 0.080893\n",
      "Epoch : 1500\n",
      "loss: -708.404785\n",
      "MSE loss: 0.022736\n",
      "Test Error: \n",
      " Avg Loss : 1065.667480 \n",
      " Test MSE loss: 0.081466\n",
      "Epoch : 1600\n",
      "loss: -779.170959\n",
      "MSE loss: 0.019914\n",
      "Test Error: \n",
      " Avg Loss : 1332.941528 \n",
      " Test MSE loss: 0.086881\n",
      "Epoch : 1700\n",
      "loss: -882.364380\n",
      "MSE loss: 0.017644\n",
      "Test Error: \n",
      " Avg Loss : 1694.025879 \n",
      " Test MSE loss: 0.082609\n",
      "Epoch : 1800\n",
      "loss: -911.831421\n",
      "MSE loss: 0.017208\n",
      "Test Error: \n",
      " Avg Loss : 1972.824219 \n",
      " Test MSE loss: 0.084522\n",
      "Epoch : 1900\n",
      "loss: -904.447998\n",
      "MSE loss: 0.018064\n",
      "Test Error: \n",
      " Avg Loss : 2342.315918 \n",
      " Test MSE loss: 0.086635\n",
      "using hyperparameters : 0.2 , 0.1\n",
      "Epoch : 0\n",
      "loss: 2269.802734\n",
      "MSE loss: 0.858101\n",
      "Test Error: \n",
      " Avg Loss : 1638.266357 \n",
      " Test MSE loss: 0.100301\n",
      "Epoch : 100\n",
      "loss: 1393.397827\n",
      "MSE loss: 0.071715\n",
      "Test Error: \n",
      " Avg Loss : 1401.412476 \n",
      " Test MSE loss: 0.079273\n",
      "Epoch : 200\n",
      "loss: 1193.857666\n",
      "MSE loss: 0.077463\n",
      "Test Error: \n",
      " Avg Loss : 1183.099487 \n",
      " Test MSE loss: 0.078441\n",
      "Epoch : 300\n",
      "loss: 984.083374\n",
      "MSE loss: 0.067381\n",
      "Test Error: \n",
      " Avg Loss : 1000.237061 \n",
      " Test MSE loss: 0.078287\n",
      "Epoch : 400\n",
      "loss: 826.045166\n",
      "MSE loss: 0.067958\n",
      "Test Error: \n",
      " Avg Loss : 814.213135 \n",
      " Test MSE loss: 0.076818\n",
      "Epoch : 500\n",
      "loss: 605.842224\n",
      "MSE loss: 0.060819\n",
      "Test Error: \n",
      " Avg Loss : 641.528809 \n",
      " Test MSE loss: 0.076405\n",
      "Epoch : 600\n",
      "loss: 454.943756\n",
      "MSE loss: 0.067754\n",
      "Test Error: \n",
      " Avg Loss : 489.817871 \n",
      " Test MSE loss: 0.075535\n",
      "Epoch : 700\n",
      "loss: 401.270691\n",
      "MSE loss: 0.075506\n",
      "Test Error: \n",
      " Avg Loss : 392.722870 \n",
      " Test MSE loss: 0.074940\n",
      "Epoch : 800\n",
      "loss: 127.414948\n",
      "MSE loss: 0.056864\n",
      "Test Error: \n",
      " Avg Loss : 303.475433 \n",
      " Test MSE loss: 0.073875\n",
      "Epoch : 900\n",
      "loss: 71.787399\n",
      "MSE loss: 0.050475\n",
      "Test Error: \n",
      " Avg Loss : 211.033920 \n",
      " Test MSE loss: 0.074072\n",
      "Epoch : 1000\n",
      "loss: 12.937721\n",
      "MSE loss: 0.055918\n",
      "Test Error: \n",
      " Avg Loss : 214.101944 \n",
      " Test MSE loss: 0.073877\n",
      "Epoch : 1100\n",
      "loss: -34.770725\n",
      "MSE loss: 0.053130\n",
      "Test Error: \n",
      " Avg Loss : 226.698395 \n",
      " Test MSE loss: 0.073995\n",
      "Epoch : 1200\n",
      "loss: -164.481476\n",
      "MSE loss: 0.047649\n",
      "Test Error: \n",
      " Avg Loss : 304.092529 \n",
      " Test MSE loss: 0.074443\n",
      "Epoch : 1300\n",
      "loss: -219.602478\n",
      "MSE loss: 0.047672\n",
      "Test Error: \n",
      " Avg Loss : 321.812500 \n",
      " Test MSE loss: 0.074940\n",
      "Epoch : 1400\n",
      "loss: -266.778687\n",
      "MSE loss: 0.041480\n",
      "Test Error: \n",
      " Avg Loss : 410.658813 \n",
      " Test MSE loss: 0.075404\n",
      "Epoch : 1500\n",
      "loss: -331.914673\n",
      "MSE loss: 0.037772\n",
      "Test Error: \n",
      " Avg Loss : 492.974976 \n",
      " Test MSE loss: 0.075265\n",
      "Epoch : 1600\n",
      "loss: -426.769531\n",
      "MSE loss: 0.034426\n",
      "Test Error: \n",
      " Avg Loss : 539.857361 \n",
      " Test MSE loss: 0.075546\n",
      "Epoch : 1700\n",
      "loss: -512.859680\n",
      "MSE loss: 0.029423\n",
      "Test Error: \n",
      " Avg Loss : 653.788757 \n",
      " Test MSE loss: 0.074997\n",
      "Epoch : 1800\n",
      "loss: -466.331604\n",
      "MSE loss: 0.030556\n",
      "Test Error: \n",
      " Avg Loss : 778.632324 \n",
      " Test MSE loss: 0.076097\n",
      "Epoch : 1900\n",
      "loss: -602.448486\n",
      "MSE loss: 0.026778\n",
      "Test Error: \n",
      " Avg Loss : 821.200989 \n",
      " Test MSE loss: 0.076067\n",
      "using hyperparameters : 0.2 , 0.2\n",
      "Epoch : 0\n",
      "loss: 1734.199829\n",
      "MSE loss: 0.223252\n",
      "Test Error: \n",
      " Avg Loss : 1646.178101 \n",
      " Test MSE loss: 0.121067\n",
      "Epoch : 100\n",
      "loss: 1394.800659\n",
      "MSE loss: 0.073982\n",
      "Test Error: \n",
      " Avg Loss : 1397.596558 \n",
      " Test MSE loss: 0.078802\n",
      "Epoch : 200\n",
      "loss: 1178.756836\n",
      "MSE loss: 0.067666\n",
      "Test Error: \n",
      " Avg Loss : 1190.965942 \n",
      " Test MSE loss: 0.078707\n",
      "Epoch : 300\n",
      "loss: 1151.047363\n",
      "MSE loss: 0.130124\n",
      "Test Error: \n",
      " Avg Loss : 1112.458130 \n",
      " Test MSE loss: 0.144623\n",
      "Epoch : 400\n",
      "loss: 822.344666\n",
      "MSE loss: 0.077099\n",
      "Test Error: \n",
      " Avg Loss : 816.086365 \n",
      " Test MSE loss: 0.076924\n",
      "Epoch : 500\n",
      "loss: 678.521790\n",
      "MSE loss: 0.082276\n",
      "Test Error: \n",
      " Avg Loss : 650.747375 \n",
      " Test MSE loss: 0.076363\n",
      "Epoch : 600\n",
      "loss: 457.674957\n",
      "MSE loss: 0.062445\n",
      "Test Error: \n",
      " Avg Loss : 494.143219 \n",
      " Test MSE loss: 0.075364\n",
      "Epoch : 700\n",
      "loss: 332.435272\n",
      "MSE loss: 0.064229\n",
      "Test Error: \n",
      " Avg Loss : 448.404602 \n",
      " Test MSE loss: 0.079129\n",
      "Epoch : 800\n",
      "loss: 271.603119\n",
      "MSE loss: 0.070368\n",
      "Test Error: \n",
      " Avg Loss : 308.679443 \n",
      " Test MSE loss: 0.076557\n",
      "Epoch : 900\n",
      "loss: 156.150970\n",
      "MSE loss: 0.062050\n",
      "Test Error: \n",
      " Avg Loss : 244.403000 \n",
      " Test MSE loss: 0.074311\n",
      "Epoch : 1000\n",
      "loss: 80.786476\n",
      "MSE loss: 0.060162\n",
      "Test Error: \n",
      " Avg Loss : 216.660965 \n",
      " Test MSE loss: 0.074275\n",
      "Epoch : 1100\n",
      "loss: 11.517646\n",
      "MSE loss: 0.057188\n",
      "Test Error: \n",
      " Avg Loss : 226.626648 \n",
      " Test MSE loss: 0.074295\n",
      "Epoch : 1200\n",
      "loss: -59.787407\n",
      "MSE loss: 0.053002\n",
      "Test Error: \n",
      " Avg Loss : 233.727127 \n",
      " Test MSE loss: 0.073936\n",
      "Epoch : 1300\n",
      "loss: -104.555489\n",
      "MSE loss: 0.051884\n",
      "Test Error: \n",
      " Avg Loss : 290.603851 \n",
      " Test MSE loss: 0.074209\n",
      "Epoch : 1400\n",
      "loss: -151.896118\n",
      "MSE loss: 0.046181\n",
      "Test Error: \n",
      " Avg Loss : 338.925720 \n",
      " Test MSE loss: 0.073879\n",
      "Epoch : 1500\n",
      "loss: -229.859222\n",
      "MSE loss: 0.041240\n",
      "Test Error: \n",
      " Avg Loss : 344.390137 \n",
      " Test MSE loss: 0.074348\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m mse_loss_fcn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     test(test_dataloader, model, t )\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[37], line 126\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, optimizer, epoch)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m#Backpropagation\u001b[39;00m\n\u001b[0;32m    125\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 126\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "lowest_test_mse_score = 1000\n",
    "mask_magnitude = 1.3\n",
    "#Search \n",
    "\n",
    "for l2_weight in [0., 0.1, 0.2, 0.3]:\n",
    "    for dropout_rate in [0., 0.1, 0.2, 0.3]:\n",
    "        print(\"using hyperparameters : \" + str( l2_weight) + \" , \" + str(dropout_rate))\n",
    "        train_dataset = MovielensDataset(user_ratings[:843], mask_magnitude)\n",
    "        test_dataset = MovielensDataset(user_ratings[843:], mask_magnitude)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "\n",
    "        losses = []\n",
    "        mse_losses = []    \n",
    "        test_losses = []\n",
    "        test_mse_losses = []\n",
    "\n",
    "        model = VariationalAutoEncoder(dropout_rate = dropout_rate).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4, weight_decay = l2_weight)\n",
    "        mse_loss_fcn = nn.MSELoss()\n",
    "\n",
    "        for t in range(epochs):\n",
    "            train(train_dataloader, model,  optimizer, t)\n",
    "            test(test_dataloader, model, t )\n",
    "            if t % 100 == 0:\n",
    "                if model.test_mse < lowest_test_mse_score:\n",
    "                    lowest_test_mse_score = model.test_mse\n",
    "                    torch.save(model.state_dict(), \"./models/best.pth\")\n",
    "                    print(\"New best using hyper parameters : \" + str(mask_magnitude) + \" ,\" + str(l2_weight) + \" ,\" + str(dropout_rate)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f9aae2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VariationalAutoEncoder:\n\tMissing key(s) in state_dict: \"log_scale\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m VariationalAutoEncoder(dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models/best.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1599\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1600\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1601\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1605\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VariationalAutoEncoder:\n\tMissing key(s) in state_dict: \"log_scale\". "
     ]
    }
   ],
   "source": [
    "model = VariationalAutoEncoder(dropout_rate = 0).to(device)\n",
    "model.load_state_dict(torch.load(\"./models/best.pth\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9530aea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5176763485477178, 0.5573624017155111, 0.5367868513897254, None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_labels = user_ratings[843:] > 0.5\n",
    "#To evaluate for comparison to other papers we mask by setting 20% of labels to false \n",
    "test_inputs = np.clip((np.random.randn(1682) + 1.35).round(), a_max = 1, a_min = 0) * user_ratings[843:]\n",
    "test_preds = model(torch.tensor(test_inputs).to(\"cuda\").to(torch.float32))[0] > 0.35\n",
    "\n",
    "precision_recall_fscore_support(relevance_labels.flatten(), test_preds.cpu().flatten(), average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b49812f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 3., ..., 8., 3., 0.],\n",
       "        [0., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 2., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " array([-1.40457118, -1.06460845, -0.72464561, -0.38468286, -0.0447201 ,\n",
       "         0.29524267,  0.63520545,  0.97516823,  1.31513095,  1.65509379,\n",
       "         1.99505651]),\n",
       " <a list of 1682 BarContainer objects>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(model(torch.tensor(test_inputs).to(\"cuda\").to(torch.float32))[0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4c539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
